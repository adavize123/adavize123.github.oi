{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Report\n",
    "## the wrangling process are sectioned into three part\n",
    "1. Gathering\n",
    "2. Assessing\n",
    "3. Cleaning \n",
    "## 1. Gathering \n",
    "#### The Gathering step include reading the first dataset ((twitter_archive_enhanced.csv) and loading it into a Dataframe and then reading the Datafram. \n",
    "#### The second dataset of the wrangling step is given in a url (https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv). which is later downloaded programatically by quering the web by using request.get, load as (image-predictions.tsv), and read from the dataframe #### The last gathering step is required to get from twitter API by quering the API using tweepy to get the tweet_json.txt from the API. I got the tweet_json.txt file from udacity classroom because i was not grant access to the API. i then read the tweet-json.txt file line by line to a dataframe ## 2. Assessing \n",
    "#### The first step of my assesment stage is done in two methods [Manual assessment and the programmatic assessment] #### the manual method is done by looking through the Dataset as follow.\n",
    "#### 1. I opened twitter_archive_enhanced.csv file in excel and i founfd the following issues.( Quality: +0000 after each timestamp value (i.e 2015-11-15 22:32:08 +0000 as time zone), some name are given in lowercase(name), invalid name entries (i.e None, a, the) and source column have unnececesary HTML code Tidiness: retweet_count and favourite_count are part of archive table,dog stage is spread in four columns(df_image)) \n",
    "#### 2. i opened the image_prediction file by reading the Dataframe \n",
    "#### 3. i also open the tweet_data of the tweet_json.txt file by the reading the Dataframe in the jupyter notebook \n",
    "#### The programmatic assessment are done using the python functions and codes to deduced the issues in the dataset. the following issues are realised (inaccurate datatype of timestamp (date type instead of object) using .info() function,rating denominator values grater than 10 , retweet count count be 0 while the favourite count is 150) \n",
    "## 3. Cleaning \n",
    "#### The cleaning process are done acording to the issues founed from the Dataset and the first step is taken to deal with the Tidiness issue. \n",
    "#### The first step: i make a copy of all my dataset and load it into a dataframe \n",
    "#### Tidiness issues are as follows: 1. retweet_count and favourite_count are part of archive table 2. dog stage is spread in four columns(df_image) \n",
    "#### 1. retweet_count and favourite_count are part of archive table is done by merging df_archive and tweet_data except tweet id column and moving the column to the archive table \n",
    "#### 2. dog stage is spread in four columns(df_image), this is done by melting the doggo, floofer,pupper,puppo column to a dog_stag as column and dropping the in other dog_stage #### Quality isuess are done by follows #### 1. removing +0000 after each timestamp value by slicing the last index string by 5 #### 2. changing the timestamp datatype to from object to date time using the to_datetime function #### 3. coverting all he first letter of the name column to capital letter using string capitalize() fucntion #### 4. replace the None, a and the as an English words with Null values by replacing the them with NaNs using replace() function initializing the english words to np.nan #### 5. drop any rating denominator value grater than 10 using the drop() funtion parsing in the parameters #### 6. drop the identcal values(duplicated) using the drop().duplicted fuction #### 7. slicing the HTML codes from the source code by droping the first 9 string and the last 37 strings #### 8. dropping the rows with the retweet count <1 using the drop() funtion #### Last step: Storing all the dataset in a single mater csv file (twitter_archive_master.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
